{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking1：什么是近似最近邻查找，常用的方法有哪些\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NN（Nearest Neighbor Search),最近邻查找问题\n",
    "- KNN(K-Nearest Neighbor Search),K近邻，查找离目标数据最近的前 K 个数据项(包括了半径和数量)\n",
    "- ANN(Approximate Nearest Neighbor),近似最近邻检索，在牺牲可接受范围的精度的情况下提高检索的效率\n",
    "- LSH, 局部敏感哈希是 ANN 的一种"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking2：为什么两个集合的minhash值相同的概率等于这两个集合的Jaccard相似度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于Ci与Cj，对应的行有三种可能：\n",
    "- A类：两列的值都为1；\n",
    "- B类：其中一列的值为0，另一列的值为1；\n",
    "- C类：两列的值都为0.\n",
    "C类行对于结果计算没有影响，可以删除\n",
    "\n",
    "\n",
    "P(h(Ci)=h(Cj))=P(删掉C类后，第一行为A类)\n",
    "=A类行的个数/所有行的个数=a/(a+b)\n",
    "\n",
    "\n",
    "P(h(Ci)=h(Cj))= Jaccard(Ci,Cj)\n",
    "\n",
    "\n",
    "用Ci，Cj的MinHash值相等的概率，对他们的Jaccard相似度进行估计\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking3：SimHash在计算文档相似度的作用是怎样的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SimHash算法高效，适用于长文本， Google 将SimHash运用到网页的去重中去，具体来说：\n",
    "\n",
    "\n",
    "互联网网页存在大量的重复内容网页，无论对于搜索引擎的网页去重和过滤、新闻小说等内容网站的内容反盗版和追踪，还是社交媒体等文本去重和聚类，都需要对网页或者文本进行去重和过滤。最简单的文本相似性计算方法可以利用空间向量模型，计算分词后的文本的特征向量的相似性，这种方法存在效率的严重弊端，无法针对海量的文本进行两两的相似性判断。模仿生物学指纹的特点，对每个文本构造一个指纹，来作为该文本的标识，从形式上来看指纹一般为固定长度较短的字符串，相同指纹的文本可以认为是相同文本。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking4：为什么YouTube采用期望观看时间作为评估指标\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 排序阶段采用观看时长作为学习目标而非点击率，由于常用的CTR指标对于视频搜索具有一定的欺骗性，比如诱导点击的标题党内容，用户点击后很快会停止观看，所以观看时长是一个更合适表示用户是否感兴趣的指标。所以作者提出采用期望观看时间作为评估指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action1：使用MinHashLSHForest对微博新闻句子进行检索 weibo.txt ,针对某句话进行Query，查找Top-3相似的句子\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH, MinHashLSHForest\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba.posseg as pseg\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件\n",
    "f = open('./weibos.txt','r', encoding = 'utf-8')\n",
    "text = f.read()\n",
    "# 以句号，叹号，问号作为分隔，去掉 \\n 换行符号\n",
    "sentences = re.split('[。 ！ ？]', text.replace('\\n', ''))\n",
    "\n",
    "# # 最后一行如果为空，则删除\n",
    "if sentences[len(sentences) - 1] == '':\n",
    "    sentences.pop()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 item_text 进行分词\n",
    "def get_item_str(item_text):\n",
    "    item_str =  ''\n",
    "    item = (pseg.cut(item_text))\n",
    "    for i in list(item):\n",
    "        # 去掉停用词\n",
    "        if i.word not in list(stop):\n",
    "            item_str += i.word\n",
    "            # tfidf_vectorizer.fit_transform 的输入需要空格分隔的单词\n",
    "            item_str += ''\n",
    "    return item_str\n",
    "\n",
    "# 对 item_str 创建 MinHash\n",
    "def get_minhash(item_str):\n",
    "    temp = MinHash()\n",
    "    for d in item_str:\n",
    "        temp.update(d.encode('utf8'))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置停用词\n",
    "stop = [line.strip() for line in open('./stopwords/cn_stopwords.txt', encoding = 'utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到停用次之后的 documents\n",
    "documents = []\n",
    "for item_text in sentences:\n",
    "    # 将 item_text 进行分词\n",
    "    item_str = get_item_str(item_text)\n",
    "    documents.append(item_str)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 LSH Forest 及 MinHash 对象\n",
    "minhash_list = []\n",
    "forest = MinHashLSHForest()\n",
    "for i in range(len(documents)):\n",
    "    # 得到 train_documents[i]的 MinHash\n",
    "    temp = get_minhash(documents[i])\n",
    "    minhash_list.append(temp)\n",
    "    forest.add(i, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index 所有 key,以便可以进行检索\n",
    "forest.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '国足昨晚-输给叙利亚赛后主帅里皮宣布辞职'\n",
    "# 将 item_text 进行分词\n",
    "item_str = get_item_str(query)\n",
    "# 得到 item_str 的 MinHash \n",
    "minhash_query = get_minhash(item_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.140625 比赛当晚太太西蒙内塔女士儿子小里皮现场台上观战\n",
      "2 0.5625 ​国足输给叙利亚之后里皮辞职\n",
      "44 1.0 国足昨晚-输给叙利亚赛后主帅里皮宣布辞职\n",
      "Top-3 邻居 [8, 2, 44]\n"
     ]
    }
   ],
   "source": [
    "# 查询 forest 中与 m1 相识的 TOP-K个邻居\n",
    "result = forest.query(minhash_query , 3)\n",
    "for i in range(len(result)):\n",
    "    print(result[i], minhash_query.jaccard(minhash_list[result[i]]), documents[result[i]].replace(' ',''))\n",
    "print('Top-3 邻居',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 2, 44]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
